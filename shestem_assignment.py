# -*- coding: utf-8 -*-
"""shestem-assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eyt9yG0pmWppbNvfeoR7f7PBxPt2fTBw
"""

import pandas as pd

# Load dataset
df = pd.read_csv("/content/traffic.csv")

# Display first 10 rows
print(df.head(10))

# Check missing values
print(df.isnull().sum())

# Handling missing values: fill numerical with median, categorical with mode
df['Vehicles'] = df['Vehicles'].fillna(df['Vehicles'].median())
df['Junction'] = df['Junction'].fillna(df['Junction'].mode()[0])
df['DateTime'] = pd.to_datetime(df['DateTime'], errors='coerce')
df['DateTime'] = df['DateTime'].fillna(method='ffill')  # forward fill

# Extract features from DateTime
df['Hour'] = df['DateTime'].dt.hour
df['DayOfWeek'] = df['DateTime'].dt.dayofweek  # Monday=0, Sunday=6
df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)

from sklearn.preprocessing import MinMaxScaler

# Select numerical features
num_cols = ['Vehicles', 'Hour']

# Apply Min-Max Normalization
scaler = MinMaxScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

print(df.head(10))

import matplotlib.pyplot as plt

# Line plot of traffic volume over time
plt.figure(figsize=(12,6))
plt.plot(df['DateTime'], df['Vehicles'], color='blue', linewidth=1)
plt.title("Traffic Volume Over Time")
plt.xlabel("DateTime")
plt.ylabel("Number of Vehicles")
plt.show()

import seaborn as sns

# Average traffic by Hour
plt.figure(figsize=(10,5))
sns.barplot(x='Hour', y='Vehicles', data=df, estimator='mean')
plt.title("Average Traffic Volume by Hour")
plt.xlabel("Hour of Day")
plt.ylabel("Average Vehicles")
plt.show()

# Average traffic by Day of Week
plt.figure(figsize=(10,5))
sns.barplot(x='DayOfWeek', y='Vehicles', data=df, estimator='mean')
plt.title("Average Traffic Volume by Day of Week")
plt.xlabel("Day of Week (0=Mon, 6=Sun)")
plt.ylabel("Average Vehicles")
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Load cleaned dataset
df = pd.read_csv("/content/traffic.csv")

# --- Identify target & features ---
# Target column = Vehicles (traffic volume)
target_col = "Vehicles"

# Convert DateTime into useful features
if "DateTime" in df.columns:
    df["DateTime"] = pd.to_datetime(df["DateTime"], errors="coerce")
    df["hour"] = df["DateTime"].dt.hour
    df["dayofweek"] = df["DateTime"].dt.dayofweek
    df["month"] = df["DateTime"].dt.month

# Drop rows with NaN target
df = df.dropna(subset=[target_col])

# Feature selection (exclude target & raw DateTime)
feature_cols = [c for c in df.columns if c not in [target_col, "DateTime"]]
X = df[feature_cols]
y = df[target_col]

# --- Train/test split (80/20) ---
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# --- Model 1: Linear Regression ---
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred_lin = lin_reg.predict(X_test)

rmse_lin = np.sqrt(mean_squared_error(y_test, y_pred_lin))
r2_lin = r2_score(y_test, y_pred_lin)

# --- Model 2: Random Forest Regressor ---
rf_reg = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
rf_reg.fit(X_train, y_train)
y_pred_rf = rf_reg.predict(X_test)

rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

# --- Results summary ---
results = pd.DataFrame([
    {"Model": "Linear Regression", "RMSE": rmse_lin, "R²": r2_lin},
    {"Model": "Random Forest", "RMSE": rmse_rf, "R²": r2_rf}
])

print(results)

# --- Plots ---
plt.figure(figsize=(12,5))

# Linear Regression
plt.subplot(1,2,1)
plt.scatter(y_test, y_pred_lin, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--")
plt.xlabel("Actual Vehicles")
plt.ylabel("Predicted Vehicles")
plt.title("Linear Regression")

# Random Forest
plt.subplot(1,2,2)
plt.scatter(y_test, y_pred_rf, alpha=0.3)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], "r--")
plt.xlabel("Actual Vehicles")
plt.ylabel("Predicted Vehicles")
plt.title("Random Forest")

plt.tight_layout()
plt.show()

# --- Feature importance from Random Forest ---
importances = rf_reg.feature_importances_
feat_imp = pd.DataFrame({
    "feature": feature_cols,
    "importance": importances
}).sort_values("importance", ascending=False)

# --- Q1: Top 3 hours with highest predicted traffic ---
# Add predictions to test set and group by 'hour'
pred_df = X_test.copy()
pred_df["actual"] = y_test
pred_df["predicted"] = y_pred_rf

# Mean predicted traffic by hour
top_hours = pred_df.groupby("hour")["predicted"].mean().sort_values(ascending=False).head(3)
print("Top 3 hours with highest predicted traffic:")
print(top_hours)

# --- Q2: Top features from Random Forest ---
top_features = feat_imp.head(5)
print("\nTop 5 important features in Random Forest model:")
print(top_features)

# --- Q3: Identify worst prediction ---
pred_df["error"] = abs(pred_df["actual"] - pred_df["predicted"])
worst_case = pred_df.loc[pred_df["error"].idxmax()]
print("\nWorst prediction example:")
print(worst_case)

# Section E: Reflection & Learning

# --- Auto-generate reflection text based on model results ---

# Reflection on what the model has learned
reflection_model = """
The Random Forest model has learned that traffic volume is strongly influenced by
the junction location and the time of day (hour). These two features dominate the
predictions, showing that traffic is not uniform but depends heavily on where and when
it occurs. The model also captured patterns like higher traffic in the evening (8 PM, 9 PM)
and mid-day (2 PM), while day of week and month had much smaller effects.
"""

# Reflection on what I personally learned
reflection_personal = """
From this exercise, I learned that traffic patterns are highly location- and time-dependent.
Simple temporal features (hour, day of week) already explain much of the variation.
Linear Regression struggled because it cannot capture non-linear patterns in the data,
but Random Forest performed very well, handling complex relationships.
Feature importance helped me understand which variables matter most (junction and hour).
I also saw that models can miss unusual traffic spikes, suggesting that adding external
factors (holidays, weather, accidents) would improve predictions further.
"""

print("=== Section E: Reflection & Learning ===\n")
print("Q1. What the model has learned (5 marks):")
print(reflection_model)

print("Q2. What I personally learned (5 marks):")
print(reflection_personal)

# Section F: Real-World Analogy & Critical Thinking

answer_real_world = """
If a city traffic planner knows the model predicts unusually high traffic at 8 AM on a Wednesday
with clear weather, they can take preventive actions such as adjusting signal timings, deploying
traffic police, or suggesting alternate routes. This moves decision-making from reactive to proactive,
similar to how a weather forecast helps people plan activities in advance.
"""

answer_reflection = """
Manual observations (like charts or averages) only show past patterns and cannot capture dynamic
conditions for a specific hour. A model prediction combines multiple features such as time, weather,
and weekday to forecast future traffic. This is valuable because it provides more accurate, forward-looking
insights that help planners anticipate and manage congestion before it happens.
"""

print("Section F: Real-World Analogy & Critical Thinking\n")
print("Real-World Analogy & Usefulness (5 marks):")
print(answer_real_world.strip())
print("\nCritical Reflection (5 marks):")
print(answer_reflection.strip())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# ============================================================
# SECTION G: Visualization & Insights
# ============================================================

# Load dataset
df = pd.read_csv("/content/traffic.csv")
df['DateTime'] = pd.to_datetime(df['DateTime'])
df = df.sort_values("DateTime")

# Focus on one junction (most frequent)
top_junction = df['Junction'].value_counts().idxmax()
d = df[df['Junction'] == top_junction].copy().reset_index(drop=True)

# Feature engineering
d['Hour'] = d['DateTime'].dt.hour
d['Day_of_Week'] = d['DateTime'].dt.day_name()
d['Weekend'] = d['Day_of_Week'].isin(['Saturday','Sunday']).astype(int)

# Time-based split (80/20)
split_idx = int(len(d) * 0.8)
train_df, test_df = d.iloc[:split_idx], d.iloc[split_idx:]

X_train = train_df[['Hour','Day_of_Week','Weekend']]
y_train = train_df['Vehicles'].values
X_test  = test_df[['Hour','Day_of_Week','Weekend']]
y_test  = test_df['Vehicles'].values

# Preprocessing + Model
pre = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), ['Day_of_Week'])],
                        remainder='passthrough')
model = Pipeline([('pre', pre), ('lr', LinearRegression())])
model.fit(X_train, y_train)

# Predictions
pred = model.predict(X_test)

# Metrics
mae = mean_absolute_error(y_test, pred)
mse = mean_squared_error(y_test, pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, pred)

print("=== Linear Regression Metrics ===")
print(f"MAE : {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R²  : {r2:.3f}")


# Plot Actual vs Predicted
plt.figure(figsize=(12,5))
plt.plot(test_df['DateTime'], y_test, label="Actual")
plt.plot(test_df['DateTime'], pred, label="Predicted")
plt.title("Actual vs Predicted Traffic Volume (Linear Regression)")
plt.xlabel("Time")
plt.ylabel("Vehicles")
plt.legend()
plt.tight_layout()
plt.show()

# ============================================================
# SHESTEM ASSIGNMENT - SECTION H
# Advanced Challenge: Sequential Model (LSTM / GRU)
# ============================================================

# Install dependencies in Colab
!pip install tensorflow scikit-learn pandas matplotlib

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense

# ============================================================
# LOAD & PREPARE DATA
# ============================================================
df = pd.read_csv("/content/traffic.csv")
df['DateTime'] = pd.to_datetime(df['DateTime'])
df = df.sort_values("DateTime")

# Focus on most frequent junction
top_junction = df['Junction'].value_counts().idxmax()
d = df[df['Junction'] == top_junction].copy().reset_index(drop=True)

# Feature engineering
d['Hour'] = d['DateTime'].dt.hour
d['Day_of_Week'] = d['DateTime'].dt.day_name()
d['Weekend'] = d['Day_of_Week'].isin(['Saturday','Sunday']).astype(int)

# ============================================================
# BASELINE: Linear Regression
# ============================================================
split_idx = int(len(d)*0.8)
train_df, test_df = d.iloc[:split_idx], d.iloc[split_idx:]

X_train = train_df[['Hour','Day_of_Week','Weekend']]
y_train = train_df['Vehicles'].values
X_test = test_df[['Hour','Day_of_Week','Weekend']]
y_test = test_df['Vehicles'].values

pre = ColumnTransformer([('cat', OneHotEncoder(handle_unknown='ignore'), ['Day_of_Week'])],
                        remainder='passthrough')
baseline = Pipeline([('pre', pre), ('lr', LinearRegression())])
baseline.fit(X_train, y_train)
y_pred_base = baseline.predict(X_test)

mae_base = mean_absolute_error(y_test, y_pred_base)
rmse_base = mean_squared_error(y_test, y_pred_base)**0.5 # Removed squared=True
r2_base = r2_score(y_test, y_pred_base)

print("=== Baseline (Linear Regression) ===")
print(f"MAE : {mae_base:.2f}")
print(f"RMSE: {rmse_base:.2f}")
print(f"R²  : {r2_base:.3f}")

# ============================================================
# SEQUENTIAL DATA PREPARATION
# ============================================================
series = d['Vehicles'].values.astype(float)

def make_sequences(data, n_past=24):
    X, y = [], []
    for i in range(len(data)-n_past):
        X.append(data[i:i+n_past])
        y.append(data[i+n_past])
    return np.array(X), np.array(y)

X_seq, y_seq = make_sequences(series, n_past=24)
split_idx = int(len(X_seq)*0.8)
X_train_seq, y_train_seq = X_seq[:split_idx], y_seq[:split_idx]
X_test_seq, y_test_seq = X_seq[split_idx:], y_seq[split_idx:]

X_train_seq = X_train_seq.reshape((X_train_seq.shape[0], X_train_seq.shape[1], 1))
X_test_seq = X_test_seq.reshape((X_test_seq.shape[0], X_test_seq.shape[1], 1))

# ============================================================
# LSTM MODEL
# ============================================================
lstm_model = Sequential([
    LSTM(64, activation='tanh', input_shape=(24,1)),
    Dense(1)
])
lstm_model.compile(optimizer='adam', loss='mse')
lstm_model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, verbose=1)

y_pred_lstm = lstm_model.predict(X_test_seq).flatten()

mae_lstm = mean_absolute_error(y_test_seq, y_pred_lstm)
rmse_lstm = mean_squared_error(y_test_seq, y_pred_lstm)**0.5 # Removed squared=True
r2_lstm = r2_score(y_test_seq, y_pred_lstm)

print("\n=== LSTM Model ===")
print(f"MAE : {mae_lstm:.2f}")
print(f"RMSE: {rmse_lstm:.2f}")
print(f"R²  : {r2_lstm:.3f}")

# ============================================================
# GRU MODEL
# ============================================================
gru_model = Sequential([
    GRU(64, activation='tanh', input_shape=(24,1)),
    Dense(1)
])
gru_model.compile(optimizer='adam', loss='mse')
gru_model.fit(X_train_seq, y_train_seq, epochs=10, batch_size=32, verbose=1)

y_pred_gru = gru_model.predict(X_test_seq).flatten()

mae_gru = mean_absolute_error(y_test_seq, y_pred_gru)
rmse_gru = mean_squared_error(y_test_seq, y_pred_gru)**0.5 # Removed squared=True
r2_gru = r2_score(y_test_seq, y_pred_gru)

print("\n=== GRU Model ===")
print(f"MAE : {mae_gru:.2f}")
print(f"RMSE: {rmse_gru:.2f}")
print(f"R²  : {r2_gru:.3f}")

# ============================================================
# VISUALIZATION
# ============================================================
plt.figure(figsize=(12,5))
plt.plot(y_test_seq[:200], label="Actual", linewidth=2)
plt.plot(y_pred_lstm[:200], label="LSTM", linestyle="--")
plt.plot(y_pred_gru[:200], label="GRU", linestyle=":")
plt.title("Traffic Prediction: Actual vs LSTM vs GRU")
plt.xlabel("Time Steps")
plt.ylabel("Vehicles")
plt.legend()
plt.tight_layout()
plt.show()

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/akhiewu/shestem-assignment.git
# %cd shestem-assignment

!python Practical Exam: Traffic Volume Prediction with Supervised ML.py